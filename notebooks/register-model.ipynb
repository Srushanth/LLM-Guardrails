{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "from nemoguardrails.llm.providers import register_llm_provider\n",
    "from nemoguardrails.llm.helpers import get_llm_instance_wrapper\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `disable_exllama` is deprecated and will be removed in version 4.37. Use `use_exllama` instead and specify the version with `exllama_config`.The value of `use_exllama` will be overwritten by `disable_exllama` passed in `GPTQConfig` or stored in your config file.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"../models/TheBloke/Mistral-7B-Instruct-v0.2-GPTQ/gptq-4bit-32g-actorder_True/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device=0)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "HFPipeline = get_llm_instance_wrapper(llm_instance=llm, llm_type=\"hf_pipeline_mistral_7b\")\n",
    "register_llm_provider(\"hf_pipeline_mistral_7b\", HFPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load rails config\n",
    "config = RailsConfig.from_path(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-02-18 21:56:28.951\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mfastembed.embedding\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m7\u001b[0m - \u001b[33m\u001b[1mDefaultEmbedding, FlagEmbedding, JinaEmbedding are deprecated. Use TextEmbedding instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered verbose mode.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bbede2571644b53b1e80976740cacbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd191e502fa4746ae5077aeebe1ba85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d4e3490d1a4d4bbb27e906427e3acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecaf4f4f9b5d408b80fe4c8baf5cf41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6c15e56e54ef4ac1cb02e3fc32ea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f962469181741bf95b44e5c622bb666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34291babd6cc4d0c89191ddcf41f953d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "617a740a1ddd45219c439fdab78f6f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5896003babf946548f43caa23a979d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07bc39c26144482fa0f36149f09d450b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create rails\n",
    "llm_rails = LLMRails(config, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST] {question} [/INST]\"\"\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "question = \"What is electroencephalography?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mUtteranceUserActionFinished\u001b[0m {'final_transcript': '{\"question\": question}'}\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mStartInternalSystemAction\u001b[0m {'uid': 'ffed11ce-db48-461d-a3f2-45f3da3edb8b', 'event_created_at': '2024-02-18T21:57:09.878608+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'create_event', 'action_params': {'event': {'_type': 'UserMessage', 'text': '$user_message'}}, 'action_result_key': None, 'action_uid': 'd8030227-98a2-4da7-bd8f-a462d721eb83', 'is_system_action': True}\u001b[0m\n",
      "\u001b[36mExecuting action\u001b[0m create_event\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mUserMessage\u001b[0m {'uid': 'bf51b689-2340-41c6-8e8a-448ce45f99d3', 'event_created_at': '2024-02-18T21:57:09.879045+00:00', 'source_uid': 'NeMoGuardrails', 'text': '{\"question\": question}'}\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mStartInternalSystemAction\u001b[0m {'uid': 'f5196990-330b-4181-906b-dc89c39b1a82', 'event_created_at': '2024-02-18T21:57:09.879484+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'generate_user_intent', 'action_params': {}, 'action_result_key': None, 'action_uid': '5d067292-26b9-494b-9248-2bb37b419246', 'is_system_action': True}\u001b[0m\n",
      "\u001b[36mExecuting action\u001b[0m generate_user_intent\u001b[0m\n",
      "Invocation Params\u001b[0m {'_type': 'hf_pipeline_mistral_7b', 'stop': None}\u001b[0m\n",
      "\u001b[34mPrompt\u001b[0m\n",
      "\u001b[38;5;232m\u001b[48;5;254m\"\"\"\n",
      "Below is a conversation between a helpful AI assistant and a user. The bot is designed to generate human-like text based on the input that it receives. The bot is talkative and provides lots of specific details. If the bot does not know the answer to a question, it truthfully says it does not know.\n",
      "\"\"\"\n",
      "\n",
      "# This is how a conversation between a user and the bot can go:\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"Tell me a bit about the history of NVIDIA.\"\n",
      "  ask general question\n",
      "bot response for general question\n",
      "  \"NVIDIA is a technology company that specializes in designing and manufacturing graphics processing units (GPUs) and other computer hardware. The company was founded in 1993 by Jen-Hsun Huang, Chris Malachowsky, and Curtis Priem.\"\n",
      "user \"tell me more\"\n",
      "  request more information\n",
      "bot provide more information\n",
      "  \"Initially, the company focused on developing 3D graphics processing technology for the PC gaming market. In 1999, NVIDIA released the GeForce 256, the world's first GPU, which was a major breakthrough for the gaming industry. The company continued to innovate in the GPU space, releasing new products and expanding into other markets such as professional graphics, mobile devices, and artificial intelligence.\"\n",
      "user \"thanks\"\n",
      "  express appreciation\n",
      "bot express appreciation and offer additional help\n",
      "  \"You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask.\"\n",
      "\n",
      "\n",
      "# This is how the user talks:\n",
      "user \"Do you know any football player?\"\n",
      "  ask sports\n",
      "\n",
      "user \"Did you see the match last night?\"\n",
      "  ask sports\n",
      "\n",
      "user \"what's up?\"\n",
      "  express greeting\n",
      "\n",
      "user \"hi\"\n",
      "  express greeting\n",
      "\n",
      "user \"hello\"\n",
      "  express greeting\n",
      "\n",
      "\n",
      "\n",
      "# This is the current conversation between the user and the bot:\n",
      "# Choose intent from this list: ask sports, express greeting\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"{\"question\": question}\"\n",
      "\u001b[0m\u001b[36mEvent\u001b[0m \u001b[38;5;32mhide_prev_turn\u001b[0m {}\u001b[0m\n",
      "\u001b[38;5;246m---\u001b[0m \u001b[38;5;246mTotal processing took 0.01 seconds.\u001b[0m\n",
      "\u001b[38;5;246m---\u001b[0m \u001b[38;5;246mStats: 1 total calls, 0 total time, 0 total tokens, 0 total prompt tokens, 0 total completion tokens\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Generate the LLM output with the guardrails applied\n",
    "output = await llm_rails.generate_async(prompt='{\"question\": question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, an internal error has occurred.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LLMRails' object has no attribute 'add_rail'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add a topical guardrail to prevent the LLM from talking about politics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mllm_rails\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_rail\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopical\u001b[39m\u001b[38;5;124m\"\u001b[39m, topics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolitics\u001b[39m\u001b[38;5;124m\"\u001b[39m], action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreject\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Add a safety guardrail to filter out unwanted language\u001b[39;00m\n\u001b[1;32m      5\u001b[0m llm_rails\u001b[38;5;241m.\u001b[39madd_rail(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msafety\u001b[39m\u001b[38;5;124m\"\u001b[39m, action\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LLMRails' object has no attribute 'add_rail'"
     ]
    }
   ],
   "source": [
    "# Add a topical guardrail to prevent the LLM from talking about politics\n",
    "llm_rails.add_rail(\"topical\", topics=[\"politics\"], action=\"reject\")\n",
    "\n",
    "# Add a safety guardrail to filter out unwanted language\n",
    "llm_rails.add_rail(\"safety\", action=\"filter\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mUtteranceUserActionFinished\u001b[0m {'final_transcript': 'hello'}\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mStartInternalSystemAction\u001b[0m {'uid': 'cab4dc85-ff98-4714-a6b9-7e4fb196799a', 'event_created_at': '2024-02-18T17:55:01.434812+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'create_event', 'action_params': {'event': {'_type': 'UserMessage', 'text': '$user_message'}}, 'action_result_key': None, 'action_uid': '3bac8469-4657-44aa-9d9b-b8797875a715', 'is_system_action': True}\u001b[0m\n",
      "\u001b[36mExecuting action\u001b[0m create_event\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mUserMessage\u001b[0m {'uid': 'b2241932-bfd8-4123-bed9-8369d5fa864b', 'event_created_at': '2024-02-18T17:55:01.435141+00:00', 'source_uid': 'NeMoGuardrails', 'text': 'hello'}\u001b[0m\n",
      "\u001b[36mEvent\u001b[0m \u001b[38;5;32mStartInternalSystemAction\u001b[0m {'uid': '27f728bb-a5ec-4544-b011-533a6770338e', 'event_created_at': '2024-02-18T17:55:01.435599+00:00', 'source_uid': 'NeMoGuardrails', 'action_name': 'generate_user_intent', 'action_params': {}, 'action_result_key': None, 'action_uid': 'e0b3219e-2402-49d8-9083-a641f625caac', 'is_system_action': True}\u001b[0m\n",
      "\u001b[36mExecuting action\u001b[0m generate_user_intent\u001b[0m\n",
      "Invocation Params\u001b[0m {'_type': 'hf_pipeline_mistral_7b', 'stop': None}\u001b[0m\n",
      "\u001b[34mPrompt\u001b[0m\n",
      "\u001b[38;5;232m\u001b[48;5;254m\"\"\"\n",
      "Below is a conversation between a helpful AI assistant and a user. The bot is designed to generate human-like text based on the input that it receives. The bot is talkative and provides lots of specific details. If the bot does not know the answer to a question, it truthfully says it does not know.\n",
      "\"\"\"\n",
      "\n",
      "# This is how a conversation between a user and the bot can go:\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"Tell me a bit about the history of NVIDIA.\"\n",
      "  ask general question\n",
      "bot response for general question\n",
      "  \"NVIDIA is a technology company that specializes in designing and manufacturing graphics processing units (GPUs) and other computer hardware. The company was founded in 1993 by Jen-Hsun Huang, Chris Malachowsky, and Curtis Priem.\"\n",
      "user \"tell me more\"\n",
      "  request more information\n",
      "bot provide more information\n",
      "  \"Initially, the company focused on developing 3D graphics processing technology for the PC gaming market. In 1999, NVIDIA released the GeForce 256, the world's first GPU, which was a major breakthrough for the gaming industry. The company continued to innovate in the GPU space, releasing new products and expanding into other markets such as professional graphics, mobile devices, and artificial intelligence.\"\n",
      "user \"thanks\"\n",
      "  express appreciation\n",
      "bot express appreciation and offer additional help\n",
      "  \"You're welcome. If you have any more questions or if there's anything else I can help you with, please don't hesitate to ask.\"\n",
      "\n",
      "\n",
      "# This is how the user talks:\n",
      "user \"Do you know any football player?\"\n",
      "  ask sports\n",
      "\n",
      "user \"Did you see the match last night?\"\n",
      "  ask sports\n",
      "\n",
      "user \"what's up?\"\n",
      "  express greeting\n",
      "\n",
      "user \"hi\"\n",
      "  express greeting\n",
      "\n",
      "user \"hello\"\n",
      "  express greeting\n",
      "\n",
      "\n",
      "\n",
      "# This is the current conversation between the user and the bot:\n",
      "# Choose intent from this list: ask sports, express greeting\n",
      "user \"Hello there!\"\n",
      "  express greeting\n",
      "bot express greeting\n",
      "  \"Hello! How can I assist you today?\"\n",
      "user \"What can you do for me?\"\n",
      "  ask about capabilities\n",
      "bot respond about capabilities\n",
      "  \"As an AI assistant, I can help you with a wide range of tasks. This includes question answering on various topics, generating text for various purposes and providing suggestions based on your preferences.\"\n",
      "user \"hello\"\n",
      "\u001b[0m\u001b[36mEvent\u001b[0m \u001b[38;5;32mhide_prev_turn\u001b[0m {}\u001b[0m\n",
      "\u001b[38;5;246m---\u001b[0m \u001b[38;5;246mTotal processing took 0.01 seconds.\u001b[0m\n",
      "\u001b[38;5;246m---\u001b[0m \u001b[38;5;246mStats: 1 total calls, 0 total time, 0 total tokens, 0 total prompt tokens, 0 total completion tokens\u001b[0m\n",
      "I'm sorry, an internal error has occurred.\n"
     ]
    }
   ],
   "source": [
    "res = await rails.generate_async(prompt=\"hello\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, an internal error has occurred.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'12.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
